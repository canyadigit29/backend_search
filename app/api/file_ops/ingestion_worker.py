import os
import asyncio
import logging
from datetime import datetime
import traceback

from fastapi import APIRouter
from app.core.supabase_client import supabase
from app.api.file_ops.ingest import process_and_embed_file

logger = logging.getLogger("ingestion_worker")
logger.setLevel(logging.INFO)

BUCKET = os.getenv("SUPABASE_STORAGE_BUCKET")

async def run_ingestion_once():
    logger.info("üîÅ Starting ingestion cycle (refactored logic)")
    print("[DEBUG] run_ingestion_once started")
    try:
        logger.info("[DEBUG] Querying files table for un-ingested files...")
        files_to_ingest = supabase.table("files").select("*").neq("ingested", True).execute()
        logger.info(f"[DEBUG] files_to_ingest query result: {files_to_ingest}")
        print(f"[DEBUG] files_to_ingest: {len(files_to_ingest.data) if files_to_ingest and files_to_ingest.data else 0}")
        if not files_to_ingest or not files_to_ingest.data:
            logger.info("üì≠ No un-ingested files found.")
            print("[DEBUG] No un-ingested files found.")
            return
        for file in files_to_ingest.data:
            file_path = file.get("file_path")
            file_id = file.get("id")
            user_id = file.get("user_id")
            print(f"[DEBUG] Processing file: {file_path}, id: {file_id}, user: {user_id}")
            if not file_path or not user_id or not file_id:
                logger.warning(f"‚ö†Ô∏è Skipping invalid row: {file}")
                print(f"[DEBUG] Skipping invalid row: {file}")
                continue
            # Directly attempt to download the file to check existence
            try:
                response = supabase.storage.from_(BUCKET).download(file_path)
                if not response:
                    logger.warning(f"üö´ File not found in storage (download failed): {file_path}")
                    print(f"[DEBUG] File not found in storage (download failed): {file_path}")
                    continue
            except Exception as e:
                logger.warning(f"üö´ Exception during file download: {file_path} - {e}")
                print(f"[DEBUG] Exception during file download: {file_path} - {e}")
                traceback.print_exc()
                continue
            logger.info(f"üßæ Ingesting: {file_path}")
            print(f"[DEBUG] Ingesting: {file_path}")
            try:
                process_file(file_path=file_path, file_id=file_id, user_id=user_id)
                print(f"[DEBUG] process_file completed for {file_path}")
            except Exception as e:
                logger.error(f"[ERROR] Exception during process_file: {e}")
                print(f"[ERROR] Exception during process_file: {e}")
                traceback.print_exc()
                continue
            try:
                logger.info(f"[DEBUG] Marking file as ingested in DB: {file_id}")
                supabase.table("files").update({
                    "ingested": True,
                    "ingested_at": datetime.utcnow().isoformat()
                }).eq("id", file_id).execute()
                print(f"[DEBUG] Marked file as ingested: {file_id}")
            except Exception as e:
                logger.error(f"[ERROR] Exception during DB update: {e}")
                print(f"[ERROR] Exception during DB update: {e}")
                traceback.print_exc()
        logger.info("‚úÖ Ingestion cycle complete")
        print("[DEBUG] run_ingestion_once completed")
    except Exception as e:
        logger.error(f"[ERROR] Exception in run_ingestion_once: {e}")
        print(f"[ERROR] Exception in run_ingestion_once: {e}")
        traceback.print_exc()


async def process_file_with_metadata(file_id: str, file_path: str, user_id: str, metadata: dict):
    """
    The new entry point for the ingestion worker, designed to be called by the background task queue.
    """
    logger.info(f"üßæ Ingesting file with metadata: {file_path}")
    try:
        # The core logic is now in process_and_embed_file, which we pass the metadata to.
        await process_and_embed_file(
            file_path=file_path,
            file_id=file_id,
            user_id=user_id,
            metadata=metadata
        )

        # Mark the file as ingested
        supabase.table("files").update({
            "ingested": True,
            "ingested_at": datetime.utcnow().isoformat()
        }).eq("id", file_id).execute()
        
        logger.info(f"‚úÖ Successfully ingested file: {file_path}")

    except Exception as e:
        logger.error(f"‚ùå Failed to ingest file {file_path}: {e}")
        traceback.print_exc()
        # Optionally, update the file record to indicate failure
        supabase.table("files").update({
            "ingested": False,
            "ingestion_error": str(e)
        }).eq("id", file_id).execute()


router = APIRouter()

@router.post("/run-ingestion")
async def run_ingestion_endpoint():
    asyncio.create_task(run_ingestion_once())
    return {"status": "Ingestion started"}

@router.post("/sync-files")
async def alias_sync_route():
    return await run_ingestion_endpoint()
